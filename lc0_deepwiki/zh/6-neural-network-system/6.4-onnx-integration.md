# ONNX 集成

相关源文件

-   [src/neural/backends/network\_onnx.cc](https://github.com/LeelaChessZero/lc0/blob/b4e98c19/src/neural/backends/network_onnx.cc)
-   [src/neural/onnx/builder.cc](https://github.com/LeelaChessZero/lc0/blob/b4e98c19/src/neural/onnx/builder.cc)
-   [src/neural/onnx/builder.h](https://github.com/LeelaChessZero/lc0/blob/b4e98c19/src/neural/onnx/builder.h)
-   [src/neural/onnx/converter.cc](https://github.com/LeelaChessZero/lc0/blob/b4e98c19/src/neural/onnx/converter.cc)
-   [src/neural/onnx/converter.h](https://github.com/LeelaChessZero/lc0/blob/b4e98c19/src/neural/onnx/converter.h)
-   [src/tools/leela2onnx.cc](https://github.com/LeelaChessZero/lc0/blob/b4e98c19/src/tools/leela2onnx.cc)
-   [src/utils/bf16\_utils.h](https://github.com/LeelaChessZero/lc0/blob/b4e98c19/src/utils/bf16_utils.h)

ONNX 集成系统通过将原生 Leela 权重格式转换为 ONNX 模型并使用 ONNX Runtime 执行它们，为 Leela Chess Zero 提供了跨平台的神经网络推理能力。该系统支持多种硬件加速提供程序和数据精度格式，从而能够在从 CPU 到专用 AI 加速器的各种计算环境中进行部署。

有关其他神经网络后端的信息，请参阅 [网络接口与后端架构](/LeelaChessZero/lc0/6.1-network-interface-and-backend-architecture)。有关权重加载和管理的详细信息，请参阅 [权重加载与管理](/LeelaChessZero/lc0/6.2-weights-loading-and-management)。

## ONNX 系统架构

ONNX 集成由两个主要子系统组成：将 Leela 权重转换为 ONNX 格式的转换流水线，以及使用各种硬件提供程序执行 ONNX 模型的运行时后端。

### ONNX 集成概览

```mermaid
flowchart TD
    LeelaWeights["Leela Weights(.pb.gz format)"]
    Converter["ConverterWeightsToOnnxConverter"]
    OnnxModel["ONNX Model(.onnx format)"]
    ConverterOptions["WeightsToOnnxConverterOptionsConfiguration"]
    OnnxNetwork["OnnxNetworkNetwork Implementation"]
    OnnxComputation["OnnxComputationBatch Processing"]
    SessionMgmt["Ort::SessionONNX Runtime Sessions"]
    CPUProvider["CPU Provideronnx-cpu"]
    CUDAProvider["CUDA Provideronnx-cuda"]
    DMLProvider["DML Provideronnx-dml"]
    ROCMProvider["ROCM Provideronnx-rocm"]
    TRTProvider["TensorRT Provideronnx-trt"]
    Float32["Float32Standard Precision"]
    Float16["Float16Half Precision"]
    BFloat16["BFloat16Brain Float"]

    LeelaWeights --> Converter
    ConverterOptions --> Converter
    Converter --> OnnxModel
    OnnxModel --> OnnxNetwork
    OnnxNetwork --> OnnxComputation
    OnnxComputation --> SessionMgmt
    SessionMgmt --> CPUProvider
    SessionMgmt --> CUDAProvider
    SessionMgmt --> DMLProvider
    SessionMgmt --> ROCMProvider
    SessionMgmt --> TRTProvider
    Converter --> Float32
    Converter --> Float16
    Converter --> BFloat16
```
来源：[src/neural/onnx/converter.h37-56](https://github.com/LeelaChessZero/lc0/blob/b4e98c19/src/neural/onnx/converter.h#L37-L56) [src/neural/backends/network\_onnx.cc86-137](https://github.com/LeelaChessZero/lc0/blob/b4e98c19/src/neural/backends/network_onnx.cc#L86-L137) [src/neural/backends/network\_onnx.cc59-60](https://github.com/LeelaChessZero/lc0/blob/b4e98c19/src/neural/backends/network_onnx.cc#L59-L60)

## 权重转换系统

转换系统通过 `Converter` 类和相关的构建器实用工具，将 Leela 的原生 protobuf 权重格式转换为 ONNX 计算图。

### 转换过程流程

```mermaid
flowchart TD
    NetPb["pblczero::NetSource Weights"]
    Options["WeightsToOnnxConverterOptionsConversion Settings"]
    ConverterClass["ConverterMain Conversion Logic"]
    OnnxBuilder["OnnxBuilderGraph Construction"]
    WeightAdapters["Weight AdaptersData Type Conversion"]
    ConvBlocks["MakeConvBlockConvolution Layers"]
    ResBlocks["MakeResidualBlockResidual Connections"]
    AttentionBody["MakeAttentionBodyTransformer Layers"]
    PolicyHead["MakePolicyHeadPolicy Output"]
    ValueHead["MakeValueHeadValue Output"]
    OnnxGraph["ONNX GraphComputational DAG"]
    SerializedModel["Serialized ModelBinary Format"]

    NetPb --> ConverterClass
    Options --> ConverterClass
    ConverterClass --> OnnxBuilder
    ConverterClass --> WeightAdapters
    OnnxBuilder --> ConvBlocks
    OnnxBuilder --> ResBlocks
    OnnxBuilder --> AttentionBody
    OnnxBuilder --> PolicyHead
    OnnxBuilder --> ValueHead
    ConvBlocks --> OnnxGraph
    ResBlocks --> OnnxGraph
    AttentionBody --> OnnxGraph
    PolicyHead --> OnnxGraph
    ValueHead --> OnnxGraph
    OnnxGraph --> SerializedModel
```
来源：[src/neural/onnx/converter.cc53-182](https://github.com/LeelaChessZero/lc0/blob/b4e98c19/src/neural/onnx/converter.cc#L53-L182) [src/neural/onnx/builder.cc39-58](https://github.com/LeelaChessZero/lc0/blob/b4e98c19/src/neural/onnx/builder.cc#L39-L58) [src/neural/onnx/converter.cc69](https://github.com/LeelaChessZero/lc0/blob/b4e98c19/src/neural/onnx/converter.cc#L69-L69)

### 转换配置选项

`WeightsToOnnxConverterOptions` 结构体为转换过程提供了广泛的配置：

| 选项 | 类型 | 目的 |
| --- | --- | --- |
| `data_type` | `DataType` | 精度格式 (Float32/Float16/BFloat16) |
| `opset` | `int` | ONNX 算子集版本 (7-22) |
| `ir` | `int` | ONNX IR 版本 (-1 为自动) |
| `batch_size` | `int` | 固定批次大小 (-1 为动态) |
| `alt_mish` | `bool` | 替代的 Mish 激活实现 |
| `alt_layernorm` | `bool` | 替代的层归一化 (LayerNormalization) 实现 |
| `policy_head` | `string` | 策略头变体 ("vanilla", "optimistic", "soft") |
| `value_head` | `string` | 价值头变体 ("winner", "q", "st") |

来源：[src/neural/onnx/converter.h38-56](https://github.com/LeelaChessZero/lc0/blob/b4e98c19/src/neural/onnx/converter.h#L38-L56) [src/tools/leela2onnx.cc146-161](https://github.com/LeelaChessZero/lc0/blob/b4e98c19/src/tools/leela2onnx.cc#L146-L161)

## ONNX Runtime 后端

运行时后端使用 ONNX Runtime 实现 `Network` 接口，提供跨多个提供程序的硬件加速推理。

### 运行时架构

```mermaid
flowchart TD
    NetworkBase["NetworkAbstract Base"]
    NetworkCapabilities["NetworkCapabilitiesFeature Detection"]
    OnnxNetwork["OnnxNetworkONNX Runtime Wrapper"]
    SessionOptions["Ort::SessionOptionsRuntime Configuration"]
    Sessions["Ort::Session[]Inference Sessions"]
    ComputationFloat32["OnnxComputation<float>32-bit Inference"]
    ComputationFloat16["OnnxComputation<Ort::Float16_t>16-bit Inference"]
    ComputationBFloat16["OnnxComputation<Ort::BFloat16_t>BFloat16 Inference"]
    InputTensor["Ort::ValueInput Tensor"]
    OutputTensors["Ort::Value[]Output Tensors"]
    BatchProcessing["Batch ProcessingDynamic Sizing"]

    NetworkBase --> OnnxNetwork
    OnnxNetwork --> NetworkCapabilities
    OnnxNetwork --> SessionOptions
    SessionOptions --> Sessions
    OnnxNetwork --> ComputationFloat32
    OnnxNetwork --> ComputationFloat16
    OnnxNetwork --> ComputationBFloat16
    ComputationFloat32 --> InputTensor
    ComputationFloat16 --> InputTensor
    ComputationBFloat16 --> InputTensor
    InputTensor --> OutputTensors
    OutputTensors --> BatchProcessing
```
来源：[src/neural/backends/network\_onnx.cc86-137](https://github.com/LeelaChessZero/lc0/blob/b4e98c19/src/neural/backends/network_onnx.cc#L86-L137) [src/neural/backends/network\_onnx.cc63-84](https://github.com/LeelaChessZero/lc0/blob/b4e98c19/src/neural/backends/network_onnx.cc#L63-L84) [src/neural/backends/network\_onnx.cc139-162](https://github.com/LeelaChessZero/lc0/blob/b4e98c19/src/neural/backends/network_onnx.cc#L139-L162)

## 硬件提供程序支持

ONNX 后端支持用于不同硬件平台的多个执行提供程序 (Execution Providers)，每个都具有特定的优化和功能。

### 提供程序架构

```mermaid
flowchart TD
    OnnxProvider["OnnxProvider enumProvider Types"]
    ProviderFactory["Provider FactoryRuntime Selection"]
    CPUProvider["CPU ProviderOrtSessionOptionsAppendExecutionProvider_CPU"]
    ThreadConfig["Thread ConfigurationSetIntraOpNumThreads"]
    CUDAProvider["CUDA ProviderOrtCUDAProviderOptions"]
    ROCMProvider["ROCM ProviderOrtROCMProviderOptions"]
    DMLProvider["DirectML ProviderOrtSessionOptionsAppendExecutionProvider_DML"]
    TensorRTProvider["TensorRT ProviderOrtTensorRTProviderOptionsV2"]
    CacheManagement["Engine Cachetrt_engine_cache_*"]
    ProfileOptimization["Profile Optimizationtrt_profile_*_shapes"]
    BatchSizing["Dynamic BatchingVariable Size Support"]
    Optimization["Graph OptimizationORT_ENABLE_ALL"]
    MemoryPattern["Memory PatternsProvider-specific"]

    OnnxProvider --> ProviderFactory
    ProviderFactory --> CPUProvider
    ProviderFactory --> CUDAProvider
    ProviderFactory --> ROCMProvider
    ProviderFactory --> DMLProvider
    ProviderFactory --> TensorRTProvider
    CPUProvider --> ThreadConfig
    TensorRTProvider --> CacheManagement
    TensorRTProvider --> ProfileOptimization
    CUDAProvider --> BatchSizing
    ROCMProvider --> Optimization
    DMLProvider --> MemoryPattern
```
来源：[src/neural/backends/network\_onnx.cc301-407](https://github.com/LeelaChessZero/lc0/blob/b4e98c19/src/neural/backends/network_onnx.cc#L301-L407) [src/neural/backends/network\_onnx.cc315-325](https://github.com/LeelaChessZero/lc0/blob/b4e98c19/src/neural/backends/network_onnx.cc#L315-L325) [src/neural/backends/network\_onnx.cc326-381](https://github.com/LeelaChessZero/lc0/blob/b4e98c19/src/neural/backends/network_onnx.cc#L326-L381)

### 提供程序配置详情

| 提供程序 | 设备支持 | 线程 | 批次模式 | 缓存支持 |
| --- | --- | --- | --- | --- |
| `onnx-cpu` | CPU | 多线程 | 动态 | 仅内存 |
| `onnx-cuda` | NVIDIA GPU | GPU 流 | 动态 | 仅内存 |
| `onnx-dml` | DirectX GPU | 顺序 | 固定 (默认 16) | 仅内存 |
| `onnx-rocm` | AMD GPU | 顺序 | 动态 | 仅内存 |
| `onnx-trt` | NVIDIA GPU | 顺序 | 动态/固定 | 引擎 + 计时 |

来源：[src/neural/backends/network\_onnx.cc516-523](https://github.com/LeelaChessZero/lc0/blob/b4e98c19/src/neural/backends/network_onnx.cc#L516-L523) [src/neural/backends/network\_onnx.cc420-424](https://github.com/LeelaChessZero/lc0/blob/b4e98c19/src/neural/backends/network_onnx.cc#L420-L424)

## 数据类型与精度支持

ONNX 集成支持多种浮点精度，并提供自动转换实用工具和特定于精度的优化。

### 数据类型转换流水线

```mermaid
flowchart TD
    FloatWeights["floatSource Weights"]
    InputPlanes["InputPlanesGame State"]
    FloatOnnxWeightsAdapter["FloatOnnxWeightsAdapterFP32 Conversion"]
    Float16OnnxWeightsAdapter["Float16OnnxWeightsAdapterFP16 Conversion"]
    BFloat16OnnxWeightsAdapter["BFloat16OnnxWeightsAdapterBF16 Conversion"]
    OrtFloat["floatONNX Runtime FP32"]
    OrtFloat16["Ort::Float16_tONNX Runtime FP16"]
    OrtBFloat16["Ort::BFloat16_tONNX Runtime BF16"]
    FP32toFP16["FP32toFP16Half Precision"]
    FP32toBF16["FP32toBF16Brain Float"]
    AsFloat["AsFloatType Extraction"]
    AsDataType["AsDataTypeType Injection"]

    FloatWeights --> FloatOnnxWeightsAdapter
    FloatWeights --> Float16OnnxWeightsAdapter
    FloatWeights --> BFloat16OnnxWeightsAdapter
    FloatOnnxWeightsAdapter --> OrtFloat
    Float16OnnxWeightsAdapter --> OrtFloat16
    BFloat16OnnxWeightsAdapter --> OrtBFloat16
    Float16OnnxWeightsAdapter --> FP32toFP16
    BFloat16OnnxWeightsAdapter --> FP32toBF16
    OrtFloat16 --> AsFloat
    OrtBFloat16 --> AsFloat
    InputPlanes --> AsDataType
```
来源：[src/neural/onnx/converter.cc197-227](https://github.com/LeelaChessZero/lc0/blob/b4e98c19/src/neural/onnx/converter.cc#L197-L227) [src/neural/backends/network\_onnx.cc173-183](https://github.com/LeelaChessZero/lc0/blob/b4e98c19/src/neural/backends/network_onnx.cc#L173-L183) [src/neural/backends/network\_onnx.cc216-224](https://github.com/LeelaChessZero/lc0/blob/b4e98c19/src/neural/backends/network_onnx.cc#L216-L224) [src/utils/bf16\_utils.h30-47](https://github.com/LeelaChessZero/lc0/blob/b4e98c19/src/utils/bf16_utils.h#L30-L47)

## 命令行转换工具

`leela2onnx` 工具提供了一个完整的接口，用于将 Leela 权重转换为 ONNX 格式，并具有广泛的配置选项。

### 工具配置矩阵

| 参数 | 值 | 默认值 | 目的 |
| --- | --- | --- | --- |
| `--onnx-opset` | 7-22 | 17 | ONNX 算子集版本 |
| `--onnx-data-type` | f32/f16/bf16 | f32 | 模型精度 |
| `--onnx-batch-size` | \-1 到 2048 | \-1 | 固定 vs 动态批处理 |
| `--policy-head` | vanilla/optimistic/soft | vanilla | 策略头变体 |
| `--value-head` | winner/q/st | winner | 价值头变体 |
| `--onnx2pytorch` | true/false | false | PyTorch 兼容模式 |

来源：[src/tools/leela2onnx.cc95-116](https://github.com/LeelaChessZero/lc0/blob/b4e98c19/src/tools/leela2onnx.cc#L95-L116) [src/tools/leela2onnx.cc146-161](https://github.com/LeelaChessZero/lc0/blob/b4e98c19/src/tools/leela2onnx.cc#L146-L161)

### 与 XLA 流水线集成

转换工具还支持为专用加速器生成 XLA HLO 表示：

```mermaid
flowchart TD
    LeelaWeights["Leela WeightsInput Format"]
    OnnxConversion["ONNX ConversionConvertWeightsToOnnx"]
    OnnxModel["ONNX ModelIntermediate Format"]
    OnnxFile["ONNX File.onnx output"]
    HloText["HLO TextHuman-readable"]
    HloProto["HLO ProtoBinary format"]
    Onnx2Hlo["ConvertOnnxToHloXLA Integration"]
    HloModule["HLO ModuleXLA Representation"]

    LeelaWeights --> OnnxConversion
    OnnxConversion --> OnnxModel
    OnnxModel --> OnnxFile
    OnnxModel --> Onnx2Hlo
    Onnx2Hlo --> HloModule
    HloModule --> HloText
    HloModule --> HloProto
```
来源：[src/tools/leela2onnx.cc168-190](https://github.com/LeelaChessZero/lc0/blob/b4e98c19/src/tools/leela2onnx.cc#L168-L190) [src/neural/xla/onnx2hlo.h](https://github.com/LeelaChessZero/lc0/blob/b4e98c19/src/neural/xla/onnx2hlo.h)
