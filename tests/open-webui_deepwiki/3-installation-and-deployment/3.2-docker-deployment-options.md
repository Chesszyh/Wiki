# Docker Deployment Options

Relevant source files

-   [LICENSE](https://github.com/open-webui/open-webui/blob/a7271532/LICENSE)
-   [README.md](https://github.com/open-webui/open-webui/blob/a7271532/README.md)
-   [TROUBLESHOOTING.md](https://github.com/open-webui/open-webui/blob/a7271532/TROUBLESHOOTING.md)
-   [docs/apache.md](https://github.com/open-webui/open-webui/blob/a7271532/docs/apache.md)
-   [src/lib/components/chat/Settings/About.svelte](https://github.com/open-webui/open-webui/blob/a7271532/src/lib/components/chat/Settings/About.svelte)
-   [src/lib/constants.ts](https://github.com/open-webui/open-webui/blob/a7271532/src/lib/constants.ts)

## Purpose and Scope

This document explains the different Docker deployment topologies available for Open WebUI, including bundled Ollama containers, separate container configurations, GPU support, and cloud-only deployments. Each topology is optimized for different use cases ranging from simple single-user setups to production multi-container orchestrations.

---

## Deployment Topology Overview

Open WebUI supports six primary deployment patterns, each suited to different requirements:

### Deployment Topology Comparison

```mermaid
flowchart TD
    G1["ghcr.io/open-webui/open-webui:cudaor :ollama with --gpus"]
    G1Vol1["GPU Access--gpus all"]
    G1Vol2["Volumes"]
    C1["ghcr.io/open-webui/open-webui:mainUI Only"]
    C1Cloud["OpenAI/Anthropic/GoogleExternal APIs"]
    C1Vol["open-webui volume"]
    S1["ghcr.io/open-webui/open-webui:mainUI ContainerPort 3000:8080"]
    S2["ollama/ollamaor External OllamaPort 11434"]
    S1Vol["open-webui volume"]
    B1["ghcr.io/open-webui/open-webui:ollamaSingle ContainerUI + Ollama Embedded"]
    B1Vol1["ollama volume/root/.ollama"]
    B1Vol2["open-webui volume/app/backend/data"]

    G1 --> G1Vol1
    G1 --> G1Vol2
    C1 --> C1Cloud
    C1 --> C1Vol
    S1 --> S2
    S1 --> S1Vol
    B1 --> B1Vol1
    B1 --> B1Vol2
```
**Sources:** [README.md108-169](https://github.com/open-webui/open-webui/blob/a7271532/README.md#L108-L169)

### When to Use Each Topology

| Topology | Best For | Considerations |
| --- | --- | --- |
| **Bundled Ollama** | Quick start, single-user, development | Simple setup, larger image size, single failure domain |
| **Separate Containers** | Production, scalability, resource isolation | Flexible deployment, independent scaling, network configuration required |
| **Cloud-Only** | No local GPU, API-first, minimal resources | Lower resource requirements, API costs, internet dependency |
| **GPU Accelerated** | Performance, large models, high throughput | Requires NVIDIA GPU, CUDA toolkit installation |
| **Kubernetes** | Enterprise, high availability, multi-tenant | Complex setup, horizontal scaling, production-grade resilience |
| **Development** | Local development, hot-reload | Frontend and backend run separately, Vite dev server |

**Sources:** [README.md86-176](https://github.com/open-webui/open-webui/blob/a7271532/README.md#L86-L176)

---

## Option 1: Bundled Ollama Deployment

The bundled deployment combines Open WebUI and Ollama in a single container using the `:ollama` image tag. This is the simplest deployment option for getting started quickly.

### Architecture: Bundled Ollama Container

```mermaid
flowchart TD
    UI["Open WebUIFastAPI + SvelteKitPort 8080"]
    OllamaEmbedded["Ollama ProcessInternal Port 11434"]
    Vol1["/root/.ollamaOllama Models"]
    Vol2["/app/backend/dataUser Data + SQLite"]
    Client["Browser"]

    UI --> OllamaEmbedded
    OllamaEmbedded --> Vol1
    UI --> Vol2
    Client --> UI
```
**Sources:** [README.md149-167](https://github.com/open-webui/open-webui/blob/a7271532/README.md#L149-L167)

### With GPU Support

```
docker run -d \
  -p 3000:8080 \
  --gpus=all \
  -v ollama:/root/.ollama \
  -v open-webui:/app/backend/data \
  --name open-webui \
  --restart always \
  ghcr.io/open-webui/open-webui:ollama
```
**Key Configuration:**

-   `--gpus=all` - Enables GPU access for Ollama model inference
-   `-v ollama:/root/.ollama` - Stores downloaded Ollama models
-   `-v open-webui:/app/backend/data` - Stores user data, chats, and SQLite database

**Sources:** [README.md153-158](https://github.com/open-webui/open-webui/blob/a7271532/README.md#L153-L158)

### CPU-Only Deployment

```
docker run -d \
  -p 3000:8080 \
  -v ollama:/root/.ollama \
  -v open-webui:/app/backend/data \
  --name open-webui \
  --restart always \
  ghcr.io/open-webui/open-webui:ollama
```
**Sources:** [README.md160-165](https://github.com/open-webui/open-webui/blob/a7271532/README.md#L160-L165)

### Bundled Deployment Characteristics

| Aspect | Configuration |
| --- | --- |
| **Image Tag** | `ghcr.io/open-webui/open-webui:ollama` |
| **Ollama Location** | Embedded within container, internal communication |
| **Port Mapping** | `3000:8080` (host:container) |
| **Model Storage** | `/root/.ollama` volume |
| **User Data** | `/app/backend/data` volume |
| **GPU Requirement** | Optional; use `--gpus=all` for GPU access |
| **Best For** | Quick start, single-user, development environments |

**Sources:** [README.md149-167](https://github.com/open-webui/open-webui/blob/a7271532/README.md#L149-L167)

---

## Option 2: Separate Container Deployment

The separate container topology runs Open WebUI and Ollama in different containers or connects to an external Ollama instance. This provides flexibility for distributed deployments and resource isolation.

### Architecture: Separate Containers

```mermaid
flowchart TD
    UI["ghcr.io/open-webui/open-webui:mainPort 3000:8080"]
    Opt1["Same Docker Hosthost.docker.internal:11434"]
    Opt2["Different Serverhttps://example.com:11434"]
    Opt3["Docker Networkollama:11434"]
    Vol["open-webui:/app/backend/data"]

    UI --> Opt1
    UI --> Opt2
    UI --> Opt3
    UI --> Vol
```
**Sources:** [README.md119-133](https://github.com/open-webui/open-webui/blob/a7271532/README.md#L119-L133) [TROUBLESHOOTING.md6-8](https://github.com/open-webui/open-webui/blob/a7271532/TROUBLESHOOTING.md#L6-L8)

### Ollama on Same Host

When Ollama runs on the same host machine:

```
docker run -d \
  -p 3000:8080 \
  --add-host=host.docker.internal:host-gateway \
  -v open-webui:/app/backend/data \
  --name open-webui \
  --restart always \
  ghcr.io/open-webui/open-webui:main
```
**Key Configuration:**

-   `--add-host=host.docker.internal:host-gateway` - Allows container to reach host machine
-   Default `OLLAMA_BASE_URL` resolves to `http://host.docker.internal:11434`
-   Ollama must be running on host at port `11434`

**Sources:** [README.md121-125](https://github.com/open-webui/open-webui/blob/a7271532/README.md#L121-L125) [TROUBLESHOOTING.md13-19](https://github.com/open-webui/open-webui/blob/a7271532/TROUBLESHOOTING.md#L13-L19)

### Ollama on Different Server

When connecting to Ollama on a remote server:

```
docker run -d \
  -p 3000:8080 \
  -e OLLAMA_BASE_URL=https://example.com \
  -v open-webui:/app/backend/data \
  --name open-webui \
  --restart always \
  ghcr.io/open-webui/open-webui:main
```
**Key Configuration:**

-   `-e OLLAMA_BASE_URL=https://example.com` - Points to remote Ollama server
-   Can use HTTP or HTTPS
-   Port defaults to `11434` if not specified

**Sources:** [README.md127-133](https://github.com/open-webui/open-webui/blob/a7271532/README.md#L127-L133) [TROUBLESHOOTING.md31-35](https://github.com/open-webui/open-webui/blob/a7271532/TROUBLESHOOTING.md#L31-L35)

### Using Host Network Mode

For troubleshooting connection issues, use host network mode:

```
docker run -d \
  --network=host \
  -v open-webui:/app/backend/data \
  -e OLLAMA_BASE_URL=http://127.0.0.1:11434 \
  --name open-webui \
  --restart always \
  ghcr.io/open-webui/open-webui:main
```
**Note:** Port changes from `3000` to `8080` when using `--network=host`. Access at `http://localhost:8080`.

**Sources:** [README.md183-189](https://github.com/open-webui/open-webui/blob/a7271532/README.md#L183-L189) [TROUBLESHOOTING.md11-19](https://github.com/open-webui/open-webui/blob/a7271532/TROUBLESHOOTING.md#L11-L19)

### Separate Container Characteristics

| Aspect | Configuration |
| --- | --- |
| **Image Tag** | `ghcr.io/open-webui/open-webui:main` |
| **Ollama Connection** | Via `OLLAMA_BASE_URL` environment variable |
| **Network Options** | Bridge (default), host, or custom network |
| **Port Mapping** | `3000:8080` (bridge) or `8080` (host mode) |
| **User Data** | `/app/backend/data` volume |
| **Best For** | Production, distributed deployments, independent scaling |

**Sources:** [README.md119-133](https://github.com/open-webui/open-webui/blob/a7271532/README.md#L119-L133) [TROUBLESHOOTING.md6-19](https://github.com/open-webui/open-webui/blob/a7271532/TROUBLESHOOTING.md#L6-L19)

---

## Option 3: GPU-Accelerated Deployment

GPU acceleration enables faster inference for large models. Open WebUI provides the `:cuda` image tag with NVIDIA GPU support.

### Architecture: GPU Access

```mermaid
flowchart TD
    UI["Open WebUI"]
    GPUAccess["--gpus allCUDA Access"]
    GPU["NVIDIA GPUHardware"]
    NVRuntime["NVIDIA Container Runtime"]
    Vol["open-webui volume"]

    GPUAccess --> NVRuntime
    NVRuntime --> GPU
    UI --> Vol
```
**Sources:** [README.md135-139](https://github.com/open-webui/open-webui/blob/a7271532/README.md#L135-L139)

### GPU Deployment Command

```
docker run -d \
  -p 3000:8080 \
  --gpus all \
  --add-host=host.docker.internal:host-gateway \
  -v open-webui:/app/backend/data \
  --name open-webui \
  --restart always \
  ghcr.io/open-webui/open-webui:cuda
```
**Key Configuration:**

-   `--gpus all` - Grants access to all GPUs on host
-   `ghcr.io/open-webui/open-webui:cuda` - CUDA-enabled image
-   Can also use `--gpus '"device=0,1"'` to specify specific GPUs

**Sources:** [README.md135-139](https://github.com/open-webui/open-webui/blob/a7271532/README.md#L135-L139)

### Prerequisites for GPU Support

Before deploying GPU-enabled containers, install the NVIDIA Container Toolkit:

**On Linux/WSL:**

```
# Add NVIDIA package repository
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | \
  sudo tee /etc/apt/sources.list.d/nvidia-docker.list

# Install nvidia-container-toolkit
sudo apt-get update
sudo apt-get install -y nvidia-container-toolkit

# Restart Docker
sudo systemctl restart docker
```
**Sources:** [README.md117](https://github.com/open-webui/open-webui/blob/a7271532/README.md#L117-L117)

### GPU Deployment Characteristics

| Aspect | Configuration |
| --- | --- |
| **Image Tag** | `ghcr.io/open-webui/open-webui:cuda` |
| **GPU Requirement** | NVIDIA GPU with CUDA support |
| **Runtime Requirement** | NVIDIA Container Toolkit installed on host |
| **GPU Selection** | `--gpus all` or `--gpus '"device=0,1"'` |
| **Performance** | Significantly faster inference for large models |
| **Best For** | High-throughput workloads, large models, production use |

**Sources:** [README.md117](https://github.com/open-webui/open-webui/blob/a7271532/README.md#L117-L117) [README.md135-139](https://github.com/open-webui/open-webui/blob/a7271532/README.md#L135-L139)

---

## Option 4: Cloud-Only Deployment

The cloud-only topology uses external LLM APIs (OpenAI, Anthropic, Google) without running local models, minimizing resource requirements.

### Architecture: Cloud-Only

```mermaid
flowchart TD
    UI["ghcr.io/open-webui/open-webui:mainUI Only"]
    Vol["open-webui volume"]
    OpenAI["OpenAI APIapi.openai.com"]
    Anthropic["Anthropic APIapi.anthropic.com"]
    Google["Google AIgenerativelanguage.googleapis.com"]
    Client["Browser"]

    UI --> Vol
    UI --> OpenAI
    UI --> Anthropic
    UI --> Google
    Client --> UI
```
**Sources:** [README.md141-147](https://github.com/open-webui/open-webui/blob/a7271532/README.md#L141-L147)

### Cloud-Only Deployment Command

```
docker run -d \
  -p 3000:8080 \
  -e OPENAI_API_KEY=your_secret_key \
  -v open-webui:/app/backend/data \
  --name open-webui \
  --restart always \
  ghcr.io/open-webui/open-webui:main
```
**Key Configuration:**

-   `-e OPENAI_API_KEY=your_secret_key` - OpenAI API authentication
-   No `OLLAMA_BASE_URL` required
-   Minimal CPU/memory requirements
-   No GPU needed

**Sources:** [README.md143-147](https://github.com/open-webui/open-webui/blob/a7271532/README.md#L143-L147)

### Additional API Configuration

Beyond `OPENAI_API_KEY`, Open WebUI supports multiple LLM providers:

| Provider | Environment Variable | Example |
| --- | --- | --- |
| **OpenAI** | `OPENAI_API_KEY` | `sk-...` |
| **Anthropic** | `ANTHROPIC_API_KEY` | `sk-ant-...` |
| **Google** | `GOOGLE_API_KEY` | `AIza...` |
| **Custom Base URL** | `OPENAI_API_BASE_URL` | `https://api.groq.com/v1` |

Configure multiple providers simultaneously by setting their respective API keys.

**Sources:** [README.md32](https://github.com/open-webui/open-webui/blob/a7271532/README.md#L32-L32) [README.md143-147](https://github.com/open-webui/open-webui/blob/a7271532/README.md#L143-L147)

### Cloud-Only Characteristics

| Aspect | Configuration |
| --- | --- |
| **Image Tag** | `ghcr.io/open-webui/open-webui:main` |
| **Local Models** | None - uses external APIs only |
| **API Keys Required** | Yes - at least one provider |
| **Resource Requirements** | Minimal - no GPU, low CPU/memory |
| **Cost Model** | Pay-per-API-call to LLM providers |
| **Best For** | No local GPU, API-first architecture, minimal infrastructure |

**Sources:** [README.md143-147](https://github.com/open-webui/open-webui/blob/a7271532/README.md#L143-L147)

---

## Multi-Container Orchestration with Docker Compose

Docker Compose enables multi-container deployments with explicit service dependencies and network isolation. The integration test workflow demonstrates a complete multi-service stack.

```mermaid
flowchart TD
    WebUI["open-webuiPort 3000:8080"]
    PG["postgresPort 5432Health Check: pg_isready"]
    OllamaService["ollamaPort 11434Volume: ollama-data"]
    RedisService["redisPort 6379"]
    ChromaService["chromadb/chromaPort 8000Persistent Vector Store"]
    OWUIData["open-webui-dataSQLite/Uploads/Cache"]
    OllamaData["ollama-dataModels"]
    ChromaData["chroma-dataEmbeddings"]
    PGData["postgres-dataPostgreSQL Data"]

    WebUI --> PG
    WebUI --> RedisService
    WebUI --> OllamaService
    WebUI --> ChromaService
    WebUI --> OWUIData
    OllamaService --> OllamaData
    ChromaService --> ChromaData
    PG --> PGData
```
### Docker Compose Architecture

```mermaid
flowchart TD
    UI["ghcr.io/open-webui/open-webui:mainPort 3000:8080"]
    PG["postgres:16Port 5432"]
    PGHealth["Health Check:pg_isready"]
    Redis["redis:7-alpinePort 6379"]
    Ollama["ollama/ollama:latestPort 11434"]
    Chroma["chromadb/chroma:latestPort 8000"]
    UIVol["open-webui-data"]
    PGVol["postgres-data"]
    RedisVol["redis-data"]
    OllamaVol["ollama-data"]
    ChromaVol["chroma-data"]

    PG --> PGHealth
    UI --> PG
    UI --> Redis
    UI --> Ollama
    UI --> Chroma
    UI --> UIVol
    PG --> PGVol
    Redis --> RedisVol
    Ollama --> OllamaVol
    Chroma --> ChromaVol
```
**Sources:** [.github/workflows/integration-test.disabled28-34](https://github.com/open-webui/open-webui/blob/a7271532/.github/workflows/integration-test.disabled#L28-L34) [.github/workflows/integration-test.disabled108-119](https://github.com/open-webui/open-webui/blob/a7271532/.github/workflows/integration-test.disabled#L108-L119)

### Production Docker Compose Example

```
services:
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    ports:
      - "3000:8080"
    volumes:
      - open-webui-data:/app/backend/data
    environment:
      - WEBUI_SECRET_KEY=${WEBUI_SECRET_KEY}
      - DATABASE_URL=postgresql://user:pass@postgres:5432/openwebui
      - REDIS_URL=redis://redis:6379
      - OLLAMA_BASE_URL=http://ollama:11434
      - CHROMA_HTTP_HOST=chromadb
      - CHROMA_HTTP_PORT=8000
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_started
    restart: always

  postgres:
    image: postgres:16
    environment:
      - POSTGRES_DB=openwebui
      - POSTGRES_USER=user
      - POSTGRES_PASSWORD=pass
    volumes:
      - postgres-data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U user"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: always

  redis:
    image: redis:7-alpine
    volumes:
      - redis-data:/data
    restart: always

  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
    restart: always

  chromadb:
    image: chromadb/chroma:latest
    ports:
      - "8000:8000"
    volumes:
      - chroma-data:/chroma/chroma
    restart: always

volumes:
  open-webui-data:
  postgres-data:
  redis-data:
  ollama-data:
  chroma-data:
```
### Docker Compose Service Discovery

Services within the Compose network discover each other by service name:

| Service Name | Resolution | Used For |
| --- | --- | --- |
| `postgres` | `postgres:5432` | `DATABASE_URL=postgresql://user:pass@postgres:5432/db` |
| `redis` | `redis:6379` | `REDIS_URL=redis://redis:6379` |
| `ollama` | `ollama:11434` | `OLLAMA_BASE_URL=http://ollama:11434` |
| `chromadb` | `chromadb:8000` | `CHROMA_HTTP_HOST=chromadb`, `CHROMA_HTTP_PORT=8000` |

**Sources:** [.github/workflows/integration-test.disabled28-34](https://github.com/open-webui/open-webui/blob/a7271532/.github/workflows/integration-test.disabled#L28-L34) [.github/workflows/integration-test.disabled108-119](https://github.com/open-webui/open-webui/blob/a7271532/.github/workflows/integration-test.disabled#L108-L119)

### Minimal Docker Compose (UI + Ollama Only)

For simpler deployments without external databases:

```
services:
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    ports:
      - "3000:8080"
    volumes:
      - open-webui-data:/app/backend/data
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
    depends_on:
      - ollama
    restart: always

  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
    restart: always

volumes:
  open-webui-data:
  ollama-data:
```
This configuration uses SQLite (default) and local file storage, suitable for development or small-scale deployments.

**Sources:** [.github/workflows/integration-test.disabled28-34](https://github.com/open-webui/open-webui/blob/a7271532/.github/workflows/integration-test.disabled#L28-L34)

---

## Container Environment Configuration

Environment variables configure container runtime behavior. For comprehensive environment variable documentation, see page 3.3 (Environment Configuration).

### Essential Environment Variables

| Variable | Purpose | Default | Example |
| --- | --- | --- | --- |
| `OLLAMA_BASE_URL` | Ollama API endpoint | `http://localhost:11434` | `http://ollama:11434` |
| `OPENAI_API_KEY` | OpenAI API authentication | None | `sk-...` |
| `WEBUI_SECRET_KEY` | JWT signing key | Auto-generated | `openssl rand -hex 32` |
| `DATABASE_URL` | Database connection | SQLite | `postgresql://user:pass@host:5432/db` |
| `REDIS_URL` | Redis connection | None (optional) | `redis://redis:6379/0` |

**Sources:** [README.md129](https://github.com/open-webui/open-webui/blob/a7271532/README.md#L129-L129) [README.md146](https://github.com/open-webui/open-webui/blob/a7271532/README.md#L146-L146) [TROUBLESHOOTING.md32](https://github.com/open-webui/open-webui/blob/a7271532/TROUBLESHOOTING.md#L32-L32)

### LLM Provider Configuration

| Variable | Purpose | Default |
| --- | --- | --- |
| `OLLAMA_BASE_URL` | Local Ollama connection | `http://localhost:11434` |
| `OPENAI_API_KEY` | OpenAI API access | None |
| `OPENAI_API_BASE_URL` | Custom OpenAI-compatible endpoint | `https://api.openai.com/v1` |
| `ANTHROPIC_API_KEY` | Anthropic Claude access | None |
| `GOOGLE_API_KEY` | Google Gemini access | None |

**Sources:** [README.md32](https://github.com/open-webui/open-webui/blob/a7271532/README.md#L32-L32) [README.md143-147](https://github.com/open-webui/open-webui/blob/a7271532/README.md#L143-L147) [TROUBLESHOOTING.md31-35](https://github.com/open-webui/open-webui/blob/a7271532/TROUBLESHOOTING.md#L31-L35)

---

## Volume Management

Persistent volumes preserve data across container restarts, updates, and migrations.

### Container Filesystem Structure

```mermaid
flowchart TD
    DataDir["/app/backend/dataPrimary Volume"]
    OllamaDir["/root/.ollamaOllama Volume(bundled image only)"]
    DB["/app/backend/data/webui.dbSQLite Database"]
    Uploads["/app/backend/data/uploadsUser Files"]
    Chroma["/app/backend/data/chroma.sqlite3Vector Store"]
    Cache["/app/backend/data/cacheModel Cache"]
    Models["/root/.ollama/modelsDownloaded Models"]

    DataDir --> DB
    DataDir --> Uploads
    DataDir --> Chroma
    DataDir --> Cache
    OllamaDir --> Models
```
**Sources:** [README.md114](https://github.com/open-webui/open-webui/blob/a7271532/README.md#L114-L114) [README.md157-158](https://github.com/open-webui/open-webui/blob/a7271532/README.md#L157-L158) [README.md164](https://github.com/open-webui/open-webui/blob/a7271532/README.md#L164-L164) [backend/open\_webui/storage/provider.py71-74](https://github.com/open-webui/open-webui/blob/a7271532/backend/open_webui/storage/provider.py#L71-L74)

### Named Volume vs Bind Mount

**Named Volume (Recommended for Production):**

```
docker run -v open-webui:/app/backend/data ...
```
-   Docker-managed storage
-   Portable across hosts
-   Automatic cleanup with `docker volume rm`
-   Better performance on macOS and Windows

**Bind Mount (Development):**

```
docker run -v ./local-data:/app/backend/data ...
```
-   Direct host filesystem access
-   Easy to browse and backup
-   Useful for debugging and development

**Sources:** [README.md114](https://github.com/open-webui/open-webui/blob/a7271532/README.md#L114-L114) [README.md124](https://github.com/open-webui/open-webui/blob/a7271532/README.md#L124-L124)

### Critical Volume Mount Points

| Mount Point | Purpose | Required | Deployment Types |
| --- | --- | --- | --- |
| `/app/backend/data` | User data, SQLite, ChromaDB | Yes | All |
| `/root/.ollama` | Ollama models | Only for `:ollama` tag | Bundled Ollama |

### Volume Backup and Migration

**Backup a named volume:**

```
docker run --rm -v open-webui:/data -v $(pwd):/backup \
  alpine tar czf /backup/open-webui-backup.tar.gz -C /data .
```
**Restore a volume:**

```
docker run --rm -v open-webui:/data -v $(pwd):/backup \
  alpine sh -c "cd /data && tar xzf /backup/open-webui-backup.tar.gz"
```
**Sources:** [README.md114](https://github.com/open-webui/open-webui/blob/a7271532/README.md#L114-L114)

---

## Storage Backend Configuration

The `StorageProvider` abstraction in [backend/open\_webui/storage/provider.py43-61](https://github.com/open-webui/open-webui/blob/a7271532/backend/open_webui/storage/provider.py#L43-L61) supports multiple storage backends selected via `STORAGE_PROVIDER` environment variable.

### Storage Provider Selection

The `get_storage_provider` function instantiates the appropriate provider based on configuration:

```mermaid
flowchart TD
    Config["STORAGE_PROVIDEREnvironment Variable"]
    Factory["get_storage_provider()"]
    Local["LocalStorageProviderFilesystem Storage"]
    S3["S3StorageProviderAWS S3 / Compatible"]
    GCS["GCSStorageProviderGoogle Cloud Storage"]
    Azure["AzureStorageProviderAzure Blob Storage"]
    Operations["upload_file()get_file()delete_file()delete_all_files()"]

    Config --> Factory
    Factory --> Local
    Factory --> S3
    Factory --> GCS
    Factory --> Azure
    Local --> Operations
    S3 --> Operations
    GCS --> Operations
    Azure --> Operations
```
**Sources:** [backend/open\_webui/storage/provider.py362-376](https://github.com/open-webui/open-webui/blob/a7271532/backend/open_webui/storage/provider.py#L362-L376)

### S3 Storage Provider

The `S3StorageProvider` supports both AWS S3 and S3-compatible storage services (MinIO, R2, etc.). It implements two authentication strategies:

1.  **Explicit Credentials**: Using `S3_ACCESS_KEY_ID` and `S3_SECRET_ACCESS_KEY`
2.  **Workload Identity**: Using IAM roles (EC2 instance profiles, EKS service accounts, etc.)

```
# Workload identity (no explicit credentials)
if S3_ACCESS_KEY_ID and S3_SECRET_ACCESS_KEY:
    self.s3_client = boto3.client('s3', ...)
else:
    # Falls back to default AWS credential chain
    self.s3_client = boto3.client('s3', ...)
```
The provider uploads files to S3 with optional object tagging:

**Sources:** [backend/open\_webui/storage/provider.py108-224](https://github.com/open-webui/open-webui/blob/a7271532/backend/open_webui/storage/provider.py#L108-L224) [backend/open\_webui/storage/provider.py120-138](https://github.com/open-webui/open-webui/blob/a7271532/backend/open_webui/storage/provider.py#L120-L138) [backend/open\_webui/storage/provider.py148-177](https://github.com/open-webui/open-webui/blob/a7271532/backend/open_webui/storage/provider.py#L148-L177)

### GCS Storage Provider

The `GCSStorageProvider` supports Google Cloud Storage with two authentication methods:

1.  **Service Account JSON**: Using `GOOGLE_APPLICATION_CREDENTIALS_JSON`
2.  **Metadata Server**: Using default application credentials (GCE instances, GKE workload identity)

**Sources:** [backend/open\_webui/storage/provider.py226-290](https://github.com/open-webui/open-webui/blob/a7271532/backend/open_webui/storage/provider.py#L226-L290) [backend/open\_webui/storage/provider.py230-239](https://github.com/open-webui/open-webui/blob/a7271532/backend/open_webui/storage/provider.py#L230-L239)

### Azure Storage Provider

The `AzureStorageProvider` supports Azure Blob Storage with two authentication strategies:

1.  **Storage Key**: Using `AZURE_STORAGE_KEY`
2.  **Managed Identity**: Using `DefaultAzureCredential` (Azure VMs, AKS, etc.)

**Sources:** [backend/open\_webui/storage/provider.py292-360](https://github.com/open-webui/open-webui/blob/a7271532/backend/open_webui/storage/provider.py#L292-L360) [backend/open\_webui/storage/provider.py298-311](https://github.com/open-webui/open-webui/blob/a7271532/backend/open_webui/storage/provider.py#L298-L311)

---

## Database Configuration in Containers

Open WebUI supports both SQLite (default) and PostgreSQL in containerized deployments. The database type is determined by the `DATABASE_URL` environment variable.

### SQLite Container Configuration

By default, SQLite is used without configuration:

```
# SQLite (default) - no DATABASE_URL needed
docker run -v open-webui-data:/app/backend/data ...
```
Database file location: `/app/backend/data/webui.db`

### PostgreSQL Container Configuration

For production deployments, PostgreSQL is recommended:

```
docker run \
  -e DATABASE_URL=postgresql://user:pass@postgres-host:5432/openwebui \
  -e DATABASE_POOL_SIZE=10 \
  -e DATABASE_POOL_MAX_OVERFLOW=10 \
  -e DATABASE_POOL_TIMEOUT=30 \
  ...
```
The integration test demonstrates PostgreSQL health checks and connection resilience:

**Sources:** [.github/workflows/integration-test.disabled181-209](https://github.com/open-webui/open-webui/blob/a7271532/.github/workflows/integration-test.disabled#L181-L209) [.github/workflows/integration-test.disabled211-228](https://github.com/open-webui/open-webui/blob/a7271532/.github/workflows/integration-test.disabled#L211-L228)

### Connection Pool Configuration

SQLAlchemy connection pooling parameters optimize database connection management in containerized environments:

| Parameter | Environment Variable | Purpose |
| --- | --- | --- |
| Pool Size | `DATABASE_POOL_SIZE` | Initial connections (default: 10) |
| Max Overflow | `DATABASE_POOL_MAX_OVERFLOW` | Additional connections beyond pool size |
| Pool Timeout | `DATABASE_POOL_TIMEOUT` | Wait time for available connection (seconds) |

**Sources:** [.github/workflows/integration-test.disabled187-189](https://github.com/open-webui/open-webui/blob/a7271532/.github/workflows/integration-test.disabled#L187-L189)

---

## Redis Configuration in Containers

Redis is optional but recommended for multi-instance deployments. It provides distributed session storage, cache coordination, and real-time event coordination.

### Redis Container Integration

```
docker run \
  -e REDIS_URL=redis://redis-host:6379/0 \
  ...
```
Redis is used for:

-   **Session Storage**: `starsessions[redis]` backend for distributed sessions
-   **WebSocket State**: `RedisDict` for session pooling and usage tracking
-   **Task Coordination**: Redis Pub/Sub for distributed task management
-   **Configuration Cache**: `RedisConfig` for synchronized runtime configuration

### Redis Connection String Format

```
redis://[:password@]host:port[/database]
```
Examples:

-   `redis://redis:6379` - No authentication, default database (0)
-   `redis://:password@redis:6379/1` - With password, database 1
-   `rediss://redis:6379` - SSL/TLS connection

**Sources:** [backend/requirements.txt22](https://github.com/open-webui/open-webui/blob/a7271532/backend/requirements.txt#L22-L22) [backend/requirements.txt30](https://github.com/open-webui/open-webui/blob/a7271532/backend/requirements.txt#L30-L30)

---

## Network Configuration

Container networking connects Open WebUI to external services and exposes the web interface to clients.

### Port Mapping

The container exposes port `8080` internally, which is typically mapped to a different host port:

```
# Map container port 8080 to host port 3000
docker run -p 3000:8080 ...

# Map to standard HTTP port 80
docker run -p 80:8080 ...

# Bind to specific interface
docker run -p 127.0.0.1:3000:8080 ...
```
### Service Discovery in Docker Compose

Docker Compose creates a default bridge network where services discover each other by service name:

```
services:
  open-webui:
    environment:
      - DATABASE_URL=postgresql://user:pass@postgres:5432/db
      - REDIS_URL=redis://redis:6379
      - OLLAMA_BASE_URL=http://ollama:11434
```
Service names (`postgres`, `redis`, `ollama`) resolve to container IP addresses within the Compose network.

### Forwarded Headers

The uvicorn server is configured with `--forwarded-allow-ips '*'` to trust proxy headers when deployed behind reverse proxies (nginx, Traefik, etc.):

**Sources:** [.github/workflows/integration-test.disabled162](https://github.com/open-webui/open-webui/blob/a7271532/.github/workflows/integration-test.disabled#L162-L162)

---

## Health Checks and Readiness Probes

Health checks ensure services are ready before dependent services start and enable container orchestration systems to detect failures.

### Open WebUI Health Endpoints

The application exposes health check endpoints:

-   `/health` - General application health
-   `/health/db` - Database connectivity check

### PostgreSQL Health Check

The integration test demonstrates PostgreSQL health checking:

```
postgres:
  healthcheck:
    test: ["CMD-SHELL", "pg_isready -U user"]
    interval: 10s
    timeout: 5s
    retries: 5
```
**Sources:** [.github/workflows/integration-test.disabled113-117](https://github.com/open-webui/open-webui/blob/a7271532/.github/workflows/integration-test.disabled#L113-L117)

### Database Reconnection Resilience

The system handles database connection failures gracefully. The integration test verifies reconnection behavior:

```
# Terminate all PostgreSQL connections
cur.execute('SELECT pg_terminate_backend(psa.pid) ...')

# Verify service reconnects automatically
status_code = curl('http://localhost:8081/health/db')
assert status_code == 200
```
**Sources:** [.github/workflows/integration-test.disabled218-228](https://github.com/open-webui/open-webui/blob/a7271532/.github/workflows/integration-test.disabled#L218-L228)

### Service Startup Coordination

The integration test demonstrates service startup coordination with wait loops:

```
# Wait for service to become available
for i in {1..40}; do
    curl -s http://localhost:8080/api/config > /dev/null && break
    sleep 1
done
```
**Sources:** [.github/workflows/integration-test.disabled165-173](https://github.com/open-webui/open-webui/blob/a7271532/.github/workflows/integration-test.disabled#L165-L173)

---

## Container Build and Testing

The integration test workflow provides insight into container build and test strategies.

### Docker Compose Build

```
docker compose \
  --file docker-compose.yaml \
  --file docker-compose.api.yaml \
  --file docker-compose.a1111-test.yaml \
  up --detach --build
```
This demonstrates:

-   Multi-file Compose configuration for modular service definitions
-   Build-time image construction
-   Detached execution for CI/CD pipelines

**Sources:** [.github/workflows/integration-test.disabled28-34](https://github.com/open-webui/open-webui/blob/a7271532/.github/workflows/integration-test.disabled#L28-L34)

### Build Cache Management

To reclaim disk space in CI environments:

```
docker builder prune --all --force
```
**Sources:** [.github/workflows/integration-test.disabled36-38](https://github.com/open-webui/open-webui/blob/a7271532/.github/workflows/integration-test.disabled#L36-L38)

### Service Readiness Verification

```
# Wait for Ollama service
until curl --output /dev/null --silent --fail http://localhost:11434; do
    printf '.'
    sleep 1
done
```
**Sources:** [.github/workflows/integration-test.disabled40-47](https://github.com/open-webui/open-webui/blob/a7271532/.github/workflows/integration-test.disabled#L40-L47)

---

## Dependencies and Requirements

The container includes all Python dependencies from the requirements files.

### Core Dependencies

| Category | Key Packages | Purpose |
| --- | --- | --- |
| Web Framework | `fastapi==0.123.0`, `uvicorn[standard]==0.37.0` | ASGI application server |
| Database | `sqlalchemy==2.0.38`, `alembic==1.17.2` | ORM and migrations |
| Sessions | `starsessions[redis]==2.2.1` | Distributed session management |
| Real-time | `python-socketio==5.15.0`, `redis` | WebSocket and state coordination |
| Authentication | `PyJWT[crypto]==2.10.1`, `authlib==1.6.5`, `bcrypt==5.0.0` | Auth mechanisms |
| Storage | `boto3==1.41.5`, `google-cloud-storage==2.19.0`, `azure-storage-blob==12.24.1` | Cloud storage clients |
| Vector DBs | `chromadb==1.1.0`, `weaviate-client==4.17.0`, `pymilvus==2.6.4` | Embedding storage |

**Sources:** [backend/requirements.txt1-153](https://github.com/open-webui/open-webui/blob/a7271532/backend/requirements.txt#L1-L153) [pyproject.toml8-117](https://github.com/open-webui/open-webui/blob/a7271532/pyproject.toml#L8-L117)

### Minimal Dependencies

The `requirements-min.txt` defines a minimal dependency set for lightweight containers:

```
fastapi==0.123.0
uvicorn[standard]==0.37.0
pydantic==2.12.5
python-socketio==5.15.0
...
```
This minimal set excludes optional features like:

-   Advanced document loaders (Tika, Marker, Docling)
-   Multiple vector database clients
-   Cloud storage SDKs
-   OCR and document intelligence services

**Sources:** [backend/requirements-min.txt1-52](https://github.com/open-webui/open-webui/blob/a7271532/backend/requirements-min.txt#L1-L52)

### Optional Dependencies

The `pyproject.toml` defines optional dependency groups:

```
[project.optional-dependencies]
postgres = [
    "psycopg2-binary==2.9.10",
    "pgvector==0.4.1",
]

all = [
    "pymongo",
    "psycopg2-binary==2.9.9",
    "qdrant-client==1.14.3",
    "weaviate-client==4.17.0",
    ...
]
```
**Sources:** [pyproject.toml131-158](https://github.com/open-webui/open-webui/blob/a7271532/pyproject.toml#L131-L158)

---

## Testing Storage Providers in Containers

The test suite demonstrates how to verify storage provider functionality in containerized environments.

### Storage Provider Test Setup

The tests use mocking and emulation to validate storage providers without requiring actual cloud services:

```
@mock_aws  # Mocks AWS services using moto
class TestS3StorageProvider:
    def test_upload_file(self):
        self.s3_client.create_bucket(Bucket=bucket_name)
        contents, s3_file_path = Storage.upload_file(...)
```
### GCS Emulator Testing

For GCS, the tests use `gcp-storage-emulator`:

```
server = create_server("localhost", 9023, in_memory=True)
server.start()
os.environ["STORAGE_EMULATOR_HOST"] = f"http://localhost:9023"
```
### Azure Storage Testing

Azure tests use `MagicMock` to simulate the Blob Storage API without requiring actual Azure resources:

```
mock_blob_service_client = MagicMock()
mock_container_client = MagicMock()
```
**Sources:** [backend/open\_webui/test/apps/webui/storage/test\_provider.py100-189](https://github.com/open-webui/open-webui/blob/a7271532/backend/open_webui/test/apps/webui/storage/test_provider.py#L100-L189) [backend/open\_webui/test/apps/webui/storage/test\_provider.py202-292](https://github.com/open-webui/open-webui/blob/a7271532/backend/open_webui/test/apps/webui/storage/test_provider.py#L202-L292) [backend/open\_webui/test/apps/webui/storage/test\_provider.py294-436](https://github.com/open-webui/open-webui/blob/a7271532/backend/open_webui/test/apps/webui/storage/test_provider.py#L294-L436)
